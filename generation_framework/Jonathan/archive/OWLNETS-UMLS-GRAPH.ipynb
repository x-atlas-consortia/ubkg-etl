{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OWLNETS-UMLS-GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adds OWLNETS output files content to existing UMLS-Graph-Extracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Asssignnment of SAB for CUI-CUI relationships (edgelist) - typically use file name before .owl in CAPS\n",
    "OWL_SAB = 'MI'\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest OWLNETS output files, remove NaN and duplicate (keys) if they were to exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_metadata = pd.read_csv(\"OWLNETS_node_metadata.txt\", sep='\\t')\n",
    "node_metadata = node_metadata.replace({'None':np.nan})\n",
    "node_metadata = node_metadata.dropna(subset=['node_id']).drop_duplicates(subset='node_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = pd.read_csv(\"OWLNETS_relations.txt\", sep='\\t')\n",
    "relations = relations.replace({'None':np.nan})\n",
    "relations = relations.dropna(subset=['relation_id']).drop_duplicates(subset='relation_id').reset_index(drop=True)\n",
    "# handle relations with no label by inserting part after # - may warrant more robust solution or a hard stop\n",
    "relations.loc[relations['relation_label'].isnull(), 'relation_label'] = relations['relation_id'].str.split('#').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = pd.read_csv(\"OWLNETS_edgelist.txt\", sep='\\t')\n",
    "edgelist = edgelist.replace({'None':np.nan})\n",
    "edgelist = edgelist.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete self-referential edges in edgelist - CUI self-reference also avoided (later) by unique CUIs for node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = edgelist[edgelist['subject'] != edgelist['object']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define codeReplacements function - modifies known code and xref formats to CodeID format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeReplacements(x):\n",
    "   return x.str.replace('NCIT ', 'NCI ', regex=False).str.replace('MESH ', 'MSH ', regex=False).str.replace('GO ', 'GO GO:', regex=False).str.replace('NCBITaxon ', 'NCBI ', regex=False).str.replace('.*UMLS.*\\s', 'UMLS ', regex=True).str.replace('.*SNOMED.*\\s', 'SNOMEDCT_US ', regex=True).str.replace('HP ', 'HPO HP:', regex=False).str.replace('^fma','FMA ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join relation_label in edgelist, convert subClassOf to isa and space to _, CodeID formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = edgelist.merge(relations, how='left', left_on='predicate', right_on='relation_id')\n",
    "edgelist = edgelist[['subject','relation_label','object']]\n",
    "del relations\n",
    "\n",
    "edgelist.loc[(edgelist.relation_label == 'subClassOf'),'relation_label'] = 'isa'\n",
    "edgelist['relation_label'] = edgelist['relation_label'].str.replace(' ', '_')\n",
    "\n",
    "edgelist['subject'] = edgelist['subject'].str.replace(':', ' ').str.replace('#', ' ').str.replace('_', ' ').str.split('/').str[-1]\n",
    "edgelist['subject'] = codeReplacements(edgelist['subject'])\n",
    "edgelist['object'] = edgelist['object'].str.replace(':', ' ').str.replace('#', ' ').str.replace('_', ' ').str.split('/').str[-1]\n",
    "edgelist['object'] = codeReplacements(edgelist['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add inverse_ edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain inverse relations from Relations Ontology json file - address absent part_of and has_part - into 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"https://raw.githubusercontent.com/oborel/obo-relations/master/ro.json\")\n",
    "#df = pd.read_json(\"ro.json\")\n",
    "\n",
    "df = pd.DataFrame(df.graphs[0]['nodes'])\n",
    "df = df[list(df['meta'].apply(lambda x: json.dumps(x)).str.contains('inverse of '))]\n",
    "df['inverse'] = df['meta'].apply(lambda x: str(x).split(sep='inverse of ')[1].split(sep='\\'')[0])\n",
    "df = df[['lbl','inverse']]\n",
    "inverse_df = df.copy()\n",
    "inverse_df.columns = ['inverse','lbl']\n",
    "df = pd.concat([df,inverse_df], axis=0).dropna().drop_duplicates().reset_index(drop=True)\n",
    "del inverse_df\n",
    "df['lbl'] = df['lbl'].str.replace(' ', '_').str.split('/').str[-1]\n",
    "df['inverse'] = df['inverse'].str.replace(' ', '_').str.split('/').str[-1]\n",
    "if len(df[(df['inverse'] == 'part_of') | (df['lbl'] == 'part_of')]) == 0:\n",
    "    df.loc[len(df.index)] = ['has_part', 'part_of']\n",
    "    df.loc[len(df.index)] = ['part_of', 'has_part']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the inverse_ relations to edge list (results in some unknowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = edgelist.merge(df, how='left', left_on='relation_label', right_on='lbl').drop_duplicates().reset_index(drop=True)\n",
    "edgelist.drop(columns=['lbl'], inplace=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add unknown inverse_ edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist.loc[edgelist['inverse'].isnull(), 'inverse'] = 'inverse_' + edgelist['relation_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up node_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeID\n",
    "node_metadata['node_id'] = node_metadata['node_id'].str.replace(':', ' ').str.replace('#', ' ').str.replace('_', ' ').str.split('/').str[-1]\n",
    "node_metadata['node_id'] = codeReplacements(node_metadata['node_id'])\n",
    "\n",
    "# synonyms .loc of notna to control for owl with no syns\n",
    "node_metadata.loc[node_metadata['node_synonyms'].notna(), 'node_synonyms'] = node_metadata[node_metadata['node_synonyms'].notna()]['node_synonyms'].astype('str').str.split('|')\n",
    "\n",
    "# dbxref .loc of notna to control for owl with no dbxref\n",
    "node_metadata.loc[node_metadata['node_dbxrefs'].notna(), 'node_dbxrefs'] = node_metadata[node_metadata['node_dbxrefs'].notna()]['node_dbxrefs'].astype('str').str.upper().str.replace(':', ' ')\n",
    "node_metadata['node_dbxrefs'] = node_metadata['node_dbxrefs'].str.split('|')\n",
    "explode_dbxrefs = node_metadata.explode('node_dbxrefs')[['node_id','node_dbxrefs']].dropna().astype(str).drop_duplicates().reset_index(drop=True)\n",
    "explode_dbxrefs['node_dbxrefs'] = codeReplacements(explode_dbxrefs['node_dbxrefs'])\n",
    "\n",
    "# Add SAB and CODE columns\n",
    "node_metadata['SAB'] = node_metadata['node_id'].str.split(' ').str[0]\n",
    "node_metadata['CODE'] = node_metadata['node_id'].str.split(' ').str[-1]\n",
    "del node_metadata['node_namespace']\n",
    "# del explode_dbxrefs - not deleted here because we need it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the UMLS CUIs for each node_id as nodeCUIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "explode_dbxrefs['nodeXrefCodes'] = explode_dbxrefs['node_dbxrefs'].str.split(' ').str[-1]\n",
    "\n",
    "explode_dbxrefs_UMLS = explode_dbxrefs[explode_dbxrefs['node_dbxrefs'].str.contains('UMLS C') == True].groupby('node_id', sort=False)['nodeXrefCodes'].apply(list).reset_index(name='nodeCUIs')\n",
    "node_metadata = node_metadata.merge(explode_dbxrefs_UMLS, how='left', on='node_id')\n",
    "del explode_dbxrefs_UMLS\n",
    "del explode_dbxrefs['nodeXrefCodes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the UMLS CUIs for each node_id from CUI-CODEs file as CUI_CODEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUI_CODEs = pd.read_csv(\"CUI-CODEs.csv\")\n",
    "CUI_CODEs = CUI_CODEs.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A big groupby - ran a couple minutes - changed groupby to not sort the keys to speed it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_CUIs = CUI_CODEs.groupby(':END_ID', sort=False)[':START_ID'].apply(list).reset_index(name='CUI_CODEs')\n",
    "node_metadata = node_metadata.merge(CODE_CUIs, how='left', left_on='node_id', right_on=':END_ID')\n",
    "del CODE_CUIs\n",
    "del node_metadata[':END_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column for Xref's CUIs - merge exploded_node_metadata with CUI_CODEs then group, eliminate duplicates and merge with node_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_xref_cui = explode_dbxrefs.merge(CUI_CODEs, how='inner', left_on='node_dbxrefs', right_on=':END_ID')\n",
    "node_xref_cui = node_xref_cui.groupby('node_id', sort=False)[':START_ID'].apply(list).reset_index(name='XrefCUIs')\n",
    "node_xref_cui['XrefCUIs'] = node_xref_cui['XrefCUIs'].apply(lambda x: pd.unique(x)).apply(list)\n",
    "node_metadata = node_metadata.merge(node_xref_cui, how='left', on='node_id')\n",
    "del node_xref_cui\n",
    "del explode_dbxrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column for base64 CUIs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base64it(x):\n",
    "   return [base64.urlsafe_b64encode(str(x).encode('UTF-8')).decode('ascii')]\n",
    "node_metadata['base64cui'] = node_metadata['node_id'].apply(base64it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cuis list and preferred cui to complete the node \"atoms\" (code, label, syns, xrefs, cuis, CUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create correct length lists\n",
    "node_metadata['cuis'] = node_metadata['base64cui']\n",
    "node_metadata['CUI'] = ''\n",
    "\n",
    "# join list across row\n",
    "node_metadata['cuis'] = node_metadata[['nodeCUIs', 'CUI_CODEs', 'XrefCUIs', 'base64cui']].values.tolist()\n",
    "    \n",
    "# remove nan, flatten, and remove duplicates - retains order of elements which is key to consistency\n",
    "node_metadata['cuis'] = node_metadata['cuis'].apply(lambda x: [i for i in x if i == i])\n",
    "node_metadata['cuis'] = node_metadata['cuis'].apply(lambda x: [i for row in x for i in row])\n",
    "node_metadata['cuis'] = node_metadata['cuis'].apply(lambda x: pd.unique(x)).apply(list)\n",
    "\n",
    "# iterate to select one CUI from cuis in row order - we ensure each node_id has its own distinct CUI\n",
    "# each node_id is assigned one CUI distinct from all others' CUIs to ensure no self-reference in edgelist\n",
    "node_idCUIs = []\n",
    "nmCUI = []\n",
    "for index, rows in node_metadata.iterrows():\n",
    "    addedone = False\n",
    "    for x in rows.cuis:\n",
    "        if ((x in node_idCUIs) | (addedone == True)):\n",
    "            dummy = 0\n",
    "        else:\n",
    "            nmCUI.append(x)\n",
    "            node_idCUIs.append(x)\n",
    "            addedone = True\n",
    "node_metadata['CUI'] = nmCUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join CUI from node_metadata to each edgelist subject and object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble CUI-CUIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge subject and object with their CUIs and drop the codes and add the SAB\n",
    "edgelist = edgelist.merge(node_metadata, how='left', left_on='subject', right_on='node_id')\n",
    "edgelist = edgelist[['CUI','relation_label','object','inverse']]\n",
    "edgelist.columns = ['CUI1','relation_label','object','inverse']\n",
    "\n",
    "edgelist = edgelist.merge(node_metadata, how='left', left_on='object', right_on='node_id')\n",
    "edgelist = edgelist[['CUI1','relation_label','CUI','inverse']]\n",
    "edgelist.columns = ['CUI1','relation_label','CUI2','inverse']\n",
    "\n",
    "edgelist = edgelist.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "edgelist['SAB'] = OWL_SAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test existence when appropriate in original csvs and then add data for each csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CUI-CUIs (':START_ID', ':END_ID', ':TYPE', 'SAB') (no prior-existance-check because want them in this SAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWO WRITES comment out during development\n",
    "\n",
    "# forward ones\n",
    "edgelist.columns = [':START_ID',':TYPE',':END_ID','inverse','SAB']\n",
    "edgelist[[':START_ID', ':END_ID', ':TYPE', 'SAB']].to_csv('CUI-CUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "#reverse ones\n",
    "edgelist.columns = [':END_ID','relation_label',':START_ID',':TYPE','SAB']\n",
    "edgelist[[':START_ID', ':END_ID', ':TYPE', 'SAB']].to_csv('CUI-CUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "del edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CODEs (CodeID:ID,SAB,CODE) - with existence check against CUI-CODE.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCODEs = node_metadata[['node_id','SAB','CODE','CUI_CODEs']]\n",
    "newCODEs = newCODEs[newCODEs['CUI_CODEs'].isnull()]\n",
    "newCODEs = newCODEs.drop(columns=['CUI_CODEs'])\n",
    "newCODEs = newCODEs.rename({'node_id': 'CodeID:ID'}, axis=1)\n",
    "\n",
    "newCODEs = newCODEs.dropna().drop_duplicates().reset_index(drop=True)\n",
    "# write/append - comment out during development\n",
    "newCODEs.to_csv('CODEs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "del newCODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CUIs (CUI:ID) - with existence check against CUI-CODE.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUIs = CUI_CODEs[[':START_ID']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "CUIs.columns = ['CUI:ID']\n",
    "\n",
    "newCUIs = node_metadata[['CUI']]\n",
    "newCUIs.columns = ['CUI:ID']\n",
    "\n",
    "# Here we isolate only the rows not already matching in existing files\n",
    "df = newCUIs.drop_duplicates().merge(CUIs.drop_duplicates(), on=CUIs.columns.to_list(), how='left', indicator=True)\n",
    "newCUIs = df.loc[df._merge=='left_only',df.columns!='_merge']\n",
    "newCUIs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "newCUIs = newCUIs.dropna().drop_duplicates().reset_index(drop=True)\n",
    "# write/append - comment out during development\n",
    "newCUIs.to_csv('CUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# del newCUIs - do not delete here because we need newCUIs list later\n",
    "# del CUIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CUI-CODEs (:START_ID,:END_ID) - with existence check against CUI-CODE.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last CUI in cuis is always base64 of node_id - here we grab those only if they are the selected CUI (and all CUIs)\n",
    "newCUI_CODEsCUI = node_metadata[['CUI','node_id']]\n",
    "newCUI_CODEsCUI.columns = [':START_ID',':END_ID']\n",
    "\n",
    "# Here we grab all the rest of the cuis except for last in list (excluding single-length cuis lists first)\n",
    "newCUI_CODEscuis = node_metadata[['cuis','node_id']][node_metadata['cuis'].apply(len)>1]\n",
    "newCUI_CODEscuis['cuis'] = newCUI_CODEscuis['cuis'].apply(lambda x: x[:-1])\n",
    "newCUI_CODEscuis = newCUI_CODEscuis.explode('cuis')[['cuis','node_id']]\n",
    "newCUI_CODEscuis.columns = [':START_ID',':END_ID']\n",
    "\n",
    "newCUI_CODEs = pd.concat([newCUI_CODEsCUI,newCUI_CODEscuis], axis=0).dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Here we isolate only the rows not already matching in existing files\n",
    "df = newCUI_CODEs.merge(CUI_CODEs, on=CUI_CODEs.columns.to_list(), how='left', indicator=True)\n",
    "newCUI_CODEs = df.loc[df._merge=='left_only',df.columns!='_merge']\n",
    "newCUI_CODEs = newCUI_CODEs.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# write/append - comment out during development\n",
    "newCUI_CODEs.to_csv('CUI-CODEs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "del newCUI_CODEsCUI\n",
    "del newCUI_CODEscuis\n",
    "del df\n",
    "del newCUI_CODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load SUIs from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUIs = pd.read_csv(\"SUIs.csv\")\n",
    "# SUIs supposedly unique but...discovered 5 NaN names in SUIs.csv and drop them here\n",
    "# ?? from ASCII converstion for Oracle to Pandas conversion on original UMLS-Graph-Extracts ??\n",
    "SUIs = SUIs.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write SUIs (SUI:ID,name) part 1, from label - with existence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSUIs = node_metadata.merge(SUIs, how='left', left_on='node_label', right_on='name')[['node_id','node_label','CUI','SUI:ID','name']]\n",
    "\n",
    "# for Term.name that don't join with node_label update the SUI:ID with base64 of node_label\n",
    "newSUIs.loc[(newSUIs['name'] != newSUIs['node_label']), 'SUI:ID'] = newSUIs[newSUIs['name'] != newSUIs['node_label']]['node_label'].apply(base64it).str[0]\n",
    "        \n",
    "# change field names and isolate non-matched ones (don't exist in SUIs file)\n",
    "newSUIs.columns = ['node_id','name','CUI','SUI:ID','OLDname']\n",
    "newSUIs = newSUIs[newSUIs['OLDname'].isnull()][['node_id','name','CUI','SUI:ID']]\n",
    "newSUIs = newSUIs.dropna().drop_duplicates().reset_index(drop=True)\n",
    "newSUIs = newSUIs[['SUI:ID','name']]\n",
    "\n",
    "# update the SUIs dataframe to total those that will be in SUIs.csv\n",
    "SUIs = pd.concat([SUIs,newSUIs], axis=0).reset_index(drop=True)\n",
    "\n",
    "# write out newSUIs - comment out during development\n",
    "newSUIs.to_csv('SUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# del newSUIs - not here because we use this dataframe name later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CUI-SUIs (:START_ID,:END_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the newCUIs associated metadata (CUIs are unique in node_metadata)\n",
    "newCUI_SUIs = newCUIs.merge(node_metadata, how='inner', left_on='CUI:ID', right_on='CUI')\n",
    "newCUI_SUIs = newCUI_SUIs[['node_label','CUI']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# get the SUIs matches\n",
    "newCUI_SUIs = newCUI_SUIs.merge(SUIs, how='left', left_on='node_label', right_on='name')[['CUI','SUI:ID']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "newCUI_SUIs.columns = [':START:ID',':END_ID']\n",
    "\n",
    "# write/append - comment out during development\n",
    "newCUI_SUIs.to_csv('CUI-SUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "#del newCUIs\n",
    "#del newCUI_SUIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CODE-SUIs and reduce to PT or SY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_SUIs = pd.read_csv(\"CODE-SUIs.csv\")\n",
    "CODE_SUIs = CODE_SUIs[((CODE_SUIs[':TYPE'] == 'PT') | (CODE_SUIs[':TYPE'] == 'SY'))]\n",
    "CODE_SUIs = CODE_SUIs.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CODE-SUIs (:END_ID,:START_ID,:TYPE,CUI) part 1, from label - with existence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does NOT (yet) address two different owl files asserting two different SUIs as PT with the same CUI,CodeID by choosing the first one in the build process (by comparing only three columns in the existence check) - a Code/CUI would thus have only one PT relationship (to only one SUI) so that is not guaranteed right now (its good practice in query to deduplicate anyway results - because for example even fully addressed two PT relationships could exist between a CODE and SUI if they are asserted on different CUIs) - to assert a vocabulary-specific relationship type as vocabulary-specific preferred term (an ingest parameter perhaps) one would create a PT (if it doesn't have one) and a SAB_PT - that is the solution for an SAB that wants to assert PT on someone else's Code (CCF may want this so there could be CCF_PT Terms on UBERON codes) - note that for SY later, this is not an issue because SY are expected to be multiple and so we use all four columns in the existence check there too but intend to keep that one that way.\n",
    "\n",
    "# get the SUIs matches\n",
    "newCODE_SUIs = node_metadata.merge(SUIs, how='left', left_on='node_label', right_on='name')[['SUI:ID','node_id','CUI']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "newCODE_SUIs.insert(2, ':TYPE', 'PT')\n",
    "newCODE_SUIs.columns = [':END_ID',':START_ID',':TYPE','CUI']\n",
    "\n",
    "# Here we isolate only the rows not already matching in existing files\n",
    "df = newCODE_SUIs.drop_duplicates().merge(CODE_SUIs.drop_duplicates(), on=CODE_SUIs.columns.to_list(), how='left', indicator=True)\n",
    "newCODE_SUIs = df.loc[df._merge=='left_only',df.columns!='_merge']\n",
    "newCODE_SUIs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# write out newCODE_SUIs - comment out during development\n",
    "newCODE_SUIs.to_csv('CODE-SUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# del newCODE_SUIs - will use this variable again later (though its overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write SUIs (SUI:ID,name) part 2, from synonyms - with existence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode and merge the synonyms\n",
    "explode_syns = node_metadata.explode('node_synonyms')[['node_id','node_synonyms','CUI']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "newSUIs = explode_syns.merge(SUIs, how='left', left_on='node_synonyms', right_on='name')[['node_id','node_synonyms','CUI','SUI:ID','name']]\n",
    "\n",
    "# for Term.name that don't join with node_synonyms update the SUI:ID with base64 of node_synonyms\n",
    "newSUIs.loc[(newSUIs['name'] != newSUIs['node_synonyms']), 'SUI:ID'] = newSUIs[newSUIs['name'] != newSUIs['node_synonyms']]['node_synonyms'].apply(base64it).str[0]        \n",
    "\n",
    "# change field names and isolate non-matched ones (don't exist in SUIs file)\n",
    "newSUIs.columns = ['node_id','name','CUI','SUI:ID','OLDname']\n",
    "newSUIs = newSUIs[newSUIs['OLDname'].isnull()][['node_id','name','CUI','SUI:ID']]\n",
    "newSUIs = newSUIs.dropna().drop_duplicates().reset_index(drop=True)\n",
    "newSUIs = newSUIs[['SUI:ID','name']]\n",
    "\n",
    "# update the SUIs dataframe to total those that will be in SUIs.csv\n",
    "SUIs = pd.concat([SUIs,newSUIs], axis=0).reset_index(drop=True)\n",
    "\n",
    "# write out newSUIs - comment out during development\n",
    "newSUIs.to_csv('SUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "del newSUIs \n",
    "# del explode_syns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CODE-SUIs (:END_ID,:START_ID,:TYPE,CUI) part 2, from synonyms - with existence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the SUIs matches\n",
    "newCODE_SUIs = explode_syns.merge(SUIs, how='left', left_on='node_synonyms', right_on='name')[['SUI:ID','node_id','CUI']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "newCODE_SUIs.insert(2, ':TYPE', 'SY')\n",
    "newCODE_SUIs.columns = [':END_ID',':START_ID',':TYPE','CUI']\n",
    "\n",
    "#Compare the new and old retaining only new\n",
    "df = newCODE_SUIs.drop_duplicates().merge(CODE_SUIs.drop_duplicates(), on=CODE_SUIs.columns.to_list(), how='left', indicator=True)\n",
    "newCODE_SUIs = df.loc[df._merge=='left_only',df.columns!='_merge']\n",
    "newCODE_SUIs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# write out newCODE_SUIs - comment out during development\n",
    "newCODE_SUIs.to_csv('CODE-SUIs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "del newCODE_SUIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write DEFs (ATUI:ID, SAB, DEF) and DEFrel (:END_ID, :START_ID) - with check for any DEFs and existence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if node_metadata['node_definition'].notna().values.any():\n",
    "    DEFs = pd.read_csv(\"DEFs.csv\")\n",
    "    DEFrel = pd.read_csv(\"DEFrel.csv\").rename(columns={':START_ID':'CUI', ':END_ID':'ATUI:ID'})\n",
    "    DEF_REL = DEFs.merge(DEFrel, how='inner', on='ATUI:ID')[['SAB','DEF','CUI']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "    newDEF_REL = node_metadata[['SAB','node_definition','CUI']].rename(columns={'node_definition':'DEF'})\n",
    "\n",
    "    #Compare the new and old retaining only new\n",
    "    df = newDEF_REL.drop_duplicates().merge(DEF_REL.drop_duplicates(), on=DEF_REL.columns.to_list(), how='left', indicator=True)\n",
    "    newDEF_REL = df.loc[df._merge=='left_only',df.columns!='_merge']\n",
    "    newDEF_REL.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add identifier\n",
    "    newDEF_REL['ATUI:ID'] = newDEF_REL['SAB']+\" \"+newDEF_REL['DEF']+\" \"+newDEF_REL['CUI']\n",
    "    newDEF_REL['ATUI:ID'] = newDEF_REL['ATUI:ID'].apply(base64it).str[0]\n",
    "    newDEF_REL = newDEF_REL[['ATUI:ID','SAB','DEF','CUI']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Write newDEFs\n",
    "    newDEF_REL[['ATUI:ID','SAB','DEF']].to_csv('DEFs.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    # Write newDEFrel\n",
    "    newDEF_REL[['ATUI:ID','CUI']].rename(columns={'ATUI:ID':':END_ID', 'CUI':':START_ID'}).to_csv('DEFrel.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    del DEFs\n",
    "    del DEFrel\n",
    "    del DEF_REL\n",
    "    del newDEF_REL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
